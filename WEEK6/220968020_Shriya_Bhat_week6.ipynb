{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8589a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a59879d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3dafa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Policy Iteration...\n",
      "Running Value Iteration...\n",
      "\n",
      "Results:\n",
      "Policy Iteration - Wins: 726/1000, Avg Reward: 0.7260, Time: 0.1034s\n",
      "Value Iteration  - Wins: 744/1000, Avg Reward: 0.7440, Time: 0.1195s\n",
      "\n",
      "Value Iteration performed better in terms of wins.\n",
      "Value Iteration achieved higher average reward.\n",
      "Policy Iteration was faster.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def policy_iteration(env, discount_factor=0.99, theta=1e-8, max_iterations=1000):\n",
    "    def policy_evaluation(policy, V, theta=1e-8):\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(env.observation_space.n):\n",
    "                v = V[s]\n",
    "                V[s] = sum([p * (r + discount_factor * V[s_]) for p, s_, r, _ in env.unwrapped.P[s][policy[s]]])\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "            if delta < theta:\n",
    "                break\n",
    "        return V\n",
    "\n",
    "    def policy_improvement(policy, V):\n",
    "        policy_stable = True\n",
    "        for s in range(env.observation_space.n):\n",
    "            old_action = policy[s]\n",
    "            policy[s] = max(range(env.action_space.n), \n",
    "                            key=lambda a: sum([p * (r + discount_factor * V[s_]) \n",
    "                                               for p, s_, r, _ in env.unwrapped.P[s][a]]))\n",
    "            if old_action != policy[s]:\n",
    "                policy_stable = False\n",
    "        return policy, policy_stable\n",
    "\n",
    "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        V = policy_evaluation(policy, V, theta)\n",
    "        policy, policy_stable = policy_improvement(policy, V)\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return policy, V\n",
    "\n",
    "def value_iteration(env, discount_factor=0.99, theta=1e-8, max_iterations=1000):\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    for i in range(max_iterations):\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            v = V[s]\n",
    "            V[s] = max([sum([p * (r + discount_factor * V[s_]) for p, s_, r, _ in env.unwrapped.P[s][a]])\n",
    "                        for a in range(env.action_space.n)])\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "    for s in range(env.observation_space.n):\n",
    "        policy[s] = max(range(env.action_space.n), \n",
    "                        key=lambda a: sum([p * (r + discount_factor * V[s_]) \n",
    "                                           for p, s_, r, _ in env.unwrapped.P[s][a]]))\n",
    "    return policy, V\n",
    "\n",
    "def run_episodes(env, policy, num_episodes=1000):\n",
    "    wins = 0\n",
    "    total_reward = 0\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "        if reward == 1:\n",
    "            wins += 1\n",
    "    return wins, total_reward / num_episodes\n",
    "\n",
    "def compare_methods(env_name, discount_factor=0.99, theta=1e-8, max_iterations=1000, num_episodes=1000):\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    print(f\"Running Policy Iteration...\")\n",
    "    start_time = time.time()\n",
    "    pi_policy, _ = policy_iteration(env, discount_factor, theta, max_iterations)\n",
    "    pi_time = time.time() - start_time\n",
    "    pi_wins, pi_avg_reward = run_episodes(env, pi_policy, num_episodes)\n",
    "\n",
    "    print(f\"Running Value Iteration...\")\n",
    "    start_time = time.time()\n",
    "    vi_policy, _ = value_iteration(env, discount_factor, theta, max_iterations)\n",
    "    vi_time = time.time() - start_time\n",
    "    vi_wins, vi_avg_reward = run_episodes(env, vi_policy, num_episodes)\n",
    "\n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"Policy Iteration - Wins: {pi_wins}/{num_episodes}, Avg Reward: {pi_avg_reward:.4f}, Time: {pi_time:.4f}s\")\n",
    "    print(f\"Value Iteration  - Wins: {vi_wins}/{num_episodes}, Avg Reward: {vi_avg_reward:.4f}, Time: {vi_time:.4f}s\")\n",
    "\n",
    "    if pi_wins > vi_wins:\n",
    "        print(\"\\nPolicy Iteration performed better in terms of wins.\")\n",
    "    elif vi_wins > pi_wins:\n",
    "        print(\"\\nValue Iteration performed better in terms of wins.\")\n",
    "    else:\n",
    "        print(\"\\nBoth methods performed equally in terms of wins.\")\n",
    "\n",
    "    if pi_avg_reward > vi_avg_reward:\n",
    "        print(\"Policy Iteration achieved higher average reward.\")\n",
    "    elif vi_avg_reward > pi_avg_reward:\n",
    "        print(\"Value Iteration achieved higher average reward.\")\n",
    "    else:\n",
    "        print(\"Both methods achieved equal average reward.\")\n",
    "\n",
    "    if pi_time < vi_time:\n",
    "        print(\"Policy Iteration was faster.\")\n",
    "    elif vi_time < pi_time:\n",
    "        print(\"Value Iteration was faster.\")\n",
    "    else:\n",
    "        print(\"Both methods took equal time.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compare_methods(\"FrozenLake-v1\", discount_factor=0.99, theta=1e-8, max_iterations=1000, num_episodes=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed51f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Policy Iteration...\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def policy_iteration(env, discount_factor=0.99, theta=1e-8, max_iterations=1000):\n",
    "    def policy_evaluation(policy, V, theta=1e-8):\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(env.observation_space.n):\n",
    "                v = V[s]\n",
    "                V[s] = sum([p * (r + discount_factor * V[s_]) for p, s_, r, _ in env.unwrapped.P[s][policy[s]]])\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "            if delta < theta:\n",
    "                break\n",
    "        return V\n",
    "\n",
    "    def policy_improvement(policy, V):\n",
    "        policy_stable = True\n",
    "        for s in range(env.observation_space.n):\n",
    "            old_action = policy[s]\n",
    "            policy[s] = max(range(env.action_space.n), \n",
    "                            key=lambda a: sum([p * (r + discount_factor * V[s_]) \n",
    "                                               for p, s_, r, _ in env.unwrapped.P[s][a]]))\n",
    "            if old_action != policy[s]:\n",
    "                policy_stable = False\n",
    "        return policy, policy_stable\n",
    "\n",
    "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        V = policy_evaluation(policy, V, theta)\n",
    "        policy, policy_stable = policy_improvement(policy, V)\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return policy, V\n",
    "\n",
    "def value_iteration(env, discount_factor=0.99, theta=1e-8, max_iterations=1000):\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    for i in range(max_iterations):\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            v = V[s]\n",
    "            V[s] = max([sum([p * (r + discount_factor * V[s_]) for p, s_, r, _ in env.unwrapped.P[s][a]])\n",
    "                        for a in range(env.action_space.n)])\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "    for s in range(env.observation_space.n):\n",
    "        policy[s] = max(range(env.action_space.n), \n",
    "                        key=lambda a: sum([p * (r + discount_factor * V[s_]) \n",
    "                                           for p, s_, r, _ in env.unwrapped.P[s][a]]))\n",
    "    return policy, V\n",
    "\n",
    "def run_episodes(env, policy, num_episodes=1000):\n",
    "    wins = 0\n",
    "    total_reward = 0\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "        if reward == 1:\n",
    "            wins += 1\n",
    "    return wins, total_reward / num_episodes\n",
    "\n",
    "def plot_value_function(V, title, shape=(4, 4)):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(V.reshape(shape), annot=True, cmap='coolwarm', linewidths=0.5, fmt=\".2f\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def compare_methods(env_name, discount_factor=0.99, theta=1e-8, max_iterations=1000, num_episodes=1000):\n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    print(f\"Running Policy Iteration...\")\n",
    "    start_time = time.time()\n",
    "    pi_policy, pi_V = policy_iteration(env, discount_factor, theta, max_iterations)\n",
    "    pi_time = time.time() - start_time\n",
    "    pi_wins, pi_avg_reward = run_episodes(env, pi_policy, num_episodes)\n",
    "    \n",
    "    print(f\"Running Value Iteration...\")\n",
    "    start_time = time.time()\n",
    "    vi_policy, vi_V = value_iteration(env, discount_factor, theta, max_iterations)\n",
    "    vi_time = time.time() - start_time\n",
    "    vi_wins, vi_avg_reward = run_episodes(env, vi_policy, num_episodes)\n",
    "    \n",
    "    # Plot value functions\n",
    "    plot_value_function(pi_V, \"Policy Iteration Value Function\")\n",
    "    plot_value_function(vi_V, \"Value Iteration Value Function\")\n",
    "    \n",
    "    # Bar chart comparison\n",
    "    metrics = ['Wins', 'Avg Reward', 'Time Taken']\n",
    "    pi_metrics = [pi_wins, pi_avg_reward, pi_time]\n",
    "    vi_metrics = [vi_wins, vi_avg_reward, vi_time]\n",
    "    x = np.arange(len(metrics))\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(x - 0.2, pi_metrics, 0.4, label='Policy Iteration', color='blue')\n",
    "    plt.bar(x + 0.2, vi_metrics, 0.4, label='Value Iteration', color='red')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylabel(\"Performance\")\n",
    "    plt.title(\"Comparison of Policy Iteration vs Value Iteration\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"Policy Iteration - Wins: {pi_wins}/{num_episodes}, Avg Reward: {pi_avg_reward:.4f}, Time: {pi_time:.4f}s\")\n",
    "    print(f\"Value Iteration  - Wins: {vi_wins}/{num_episodes}, Avg Reward: {vi_avg_reward:.4f}, Time: {vi_time:.4f}s\")\n",
    "\n",
    "    if pi_wins > vi_wins:\n",
    "        print(\"\\nPolicy Iteration performed better in terms of wins.\")\n",
    "    elif vi_wins > pi_wins:\n",
    "        print(\"\\nValue Iteration performed better in terms of wins.\")\n",
    "    else:\n",
    "        print(\"\\nBoth methods performed equally in terms of wins.\")\n",
    "\n",
    "    if pi_avg_reward > vi_avg_reward:\n",
    "        print(\"Policy Iteration achieved higher average reward.\")\n",
    "    elif vi_avg_reward > pi_avg_reward:\n",
    "        print(\"Value Iteration achieved higher average reward.\")\n",
    "    else:\n",
    "        print(\"Both methods achieved equal average reward.\")\n",
    "\n",
    "    if pi_time < vi_time:\n",
    "        print(\"Policy Iteration was faster.\")\n",
    "    elif vi_time < pi_time:\n",
    "        print(\"Value Iteration was faster.\")\n",
    "    else:\n",
    "        print(\"Both methods took equal time.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compare_methods(\"FrozenLake-v1\", discount_factor=0.99, theta=1e-8, max_iterations=1000, num_episodes=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8454c105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
