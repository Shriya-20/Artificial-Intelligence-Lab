{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b4987b8",
   "metadata": {},
   "source": [
    "#  Name: Shriya Bhat\n",
    "### Reg: 220968020\n",
    "### Class: DSE A1\n",
    "### Week 8 - MDP & DYNAMIC PROGRAMMING: Frozen Lake environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53408204",
   "metadata": {},
   "source": [
    "# **Frozen Lake Environment - Policy Iteration vs. Value Iteration**\n",
    "\n",
    "### **Objective**\n",
    "Learn the optimal policy for the Frozen Lake environment using **Policy Iteration** and **Value Iteration**, and compare their performance.\n",
    "\n",
    "### **Frozen Lake Environment**\n",
    "We use OpenAI Gym's Frozen Lake environment:  \n",
    "ðŸ”— [Frozen Lake - Gym Documentation](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/)  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Policy Iteration**\n",
    "### **Parameters:**\n",
    "- **Policy**: 2D array of shape (nS, nA), each cell represents the probability of taking action *a* in state *s*.\n",
    "- **Environment**: Initialized OpenAI Gym environment.\n",
    "- **Discount Factor** (*Î³*): Factor for future rewards.\n",
    "- **Theta**: Convergence threshold for value function updates.\n",
    "- **Max Iterations**: Maximum number of iterations before stopping.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Value Iteration**\n",
    "### **Parameters:**\n",
    "- **Environment**: Initialized OpenAI Gym environment.\n",
    "- **Discount Factor** (*Î³*): Factor for future rewards.\n",
    "- **Theta**: Convergence threshold for value function updates.\n",
    "- **Max Iterations**: Maximum number of iterations before stopping.\n",
    "\n",
    "###  c.  Compare the number of wins, and average return after 1000 episodes and comment on which method performed                   better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3b724bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "import numpy as np\n",
    "import time \n",
    "from gymnasium.envs.toy_text.frozen_lake import FrozenLakeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c5d9697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(policy, env, discount_factor=0.99, theta=1e-8, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Performs Policy Iteration to find an optimal policy for a given environment.\n",
    "\n",
    "    Args:\n",
    "        policy: Initial policy (probabilities of actions in each state)\n",
    "        env: OpenAI Gym environment\n",
    "        discount_factor: Discount rate for future rewards\n",
    "        theta: Convergence threshold\n",
    "        max_iterations: Max number of iterations\n",
    "\n",
    "    Returns:\n",
    "        V: Optimal state-value function\n",
    "        policy: Optimized policy\n",
    "    \"\"\"\n",
    "    if hasattr(env, 'unwrapped'):\n",
    "        env = env.unwrapped  # Get the raw environment if wrapped\n",
    "\n",
    "    nS = env.observation_space.n  # Number of states(16 for standard frozen lake)\n",
    "    nA = env.action_space.n  # Number of actions\n",
    "    V = np.zeros(nS)  # Initialize state-value function\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        # Policy Evaluation: Iteratively update state values based on current policy\n",
    "        while True:\n",
    "            delta = 0  # Track max change in state values\n",
    "            for s in range(nS):\n",
    "                v_old = V[s]  # Store old value for convergence check\n",
    "                v_new = 0  # New value initialize\n",
    "                for a, action_prob in enumerate(policy[s]):  # Loop through actions\n",
    "                    for prob, next_state, reward, done in env.P[s][a]:  # Loop through possible transitions\n",
    "                        v_new += action_prob * prob * (reward + discount_factor * V[next_state])\n",
    "                V[s] = v_new  # Update state value\n",
    "                delta = max(delta, abs(v_old - v_new))  # Check for convergence\n",
    "            if delta < theta:  # Stop if change is small\n",
    "                break\n",
    "\n",
    "        # Policy Improvement: Update policy based on new value function\n",
    "        policy_stable = True\n",
    "        for s in range(nS):\n",
    "            old_action = np.argmax(policy[s])  # Current best action\n",
    "            action_values = np.zeros(nA)  # Store action values\n",
    "            for a in range(nA):\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    action_values[a] += prob * (reward + discount_factor * V[next_state])\n",
    "            best_action = np.argmax(action_values)  # Find the best action\n",
    "            new_policy = np.eye(nA)[best_action]  # Update policy for best action\n",
    "            if not np.array_equal(new_policy, policy[s]):  # Check if policy changed\n",
    "                policy_stable = False\n",
    "            policy[s] = new_policy  # Apply new policy\n",
    "\n",
    "        if policy_stable:  # Stop if policy is stable\n",
    "            break\n",
    "\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abf1c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, discount_factor=0.99, theta=1e-8, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Performs Value Iteration to find an optimal policy.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI Gym environment\n",
    "        discount_factor: Discount rate for future rewards\n",
    "        theta: Convergence threshold\n",
    "        max_iterations: Max number of iterations\n",
    "\n",
    "    Returns:\n",
    "        V: Optimal state-value function\n",
    "        policy: Optimal policy\n",
    "    \"\"\"\n",
    "    if hasattr(env, 'unwrapped'):\n",
    "        env = env.unwrapped  # Get raw environment\n",
    "\n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n\n",
    "    V = np.zeros(nS)  # Initialize state-value function\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        delta = 0  # Track max value change\n",
    "        for s in range(nS):\n",
    "            v_old = V[s]  # Store old value\n",
    "            action_values = np.zeros(nA)  # Store action values\n",
    "            for a in range(nA):\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    action_values[a] += prob * (reward + discount_factor * V[next_state])\n",
    "            V[s] = np.max(action_values)  # Update state value\n",
    "            delta = max(delta, abs(v_old - V[s]))  # Check for convergence\n",
    "        if delta < theta:  # Stop if change is small\n",
    "            break\n",
    "\n",
    "    # Derive policy from optimal value function\n",
    "    policy = np.zeros((nS, nA))\n",
    "    for s in range(nS):\n",
    "        action_values = np.zeros(nA)  # Compute action values\n",
    "        for a in range(nA):\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                action_values[a] += prob * (reward + discount_factor * V[next_state])\n",
    "        best_action = np.argmax(action_values)  # Select best action\n",
    "        policy[s] = np.eye(nA)[best_action]  # One-hot encode policy\n",
    "\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c97dae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, num_episodes=1000):\n",
    "    \"\"\"\n",
    "    Evaluates a given policy by running multiple episodes and calculating win rate and average return.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI Gym environment\n",
    "        policy: Policy to evaluate\n",
    "        num_episodes: Number of episodes to simulate\n",
    "\n",
    "    Returns:\n",
    "        wins: Number of successful episodes (goal reached)\n",
    "        avg_return: Average return per episode\n",
    "    \"\"\"\n",
    "    wins = 0\n",
    "    total_return = 0\n",
    "    for i in range(num_episodes):\n",
    "        state, _ = env.reset()  # Reset environment\n",
    "        terminated = truncated = False\n",
    "        episode_return = 0\n",
    "        while not (terminated or truncated):  # Run until episode ends\n",
    "            action = np.random.choice(np.arange(env.action_space.n), p=policy[state])  # Choose action from policy\n",
    "            state, reward, terminated, truncated, _ = env.step(action)  # Take action\n",
    "            episode_return += reward  # Accumulate reward\n",
    "        if reward > 0:  # Check if goal was reached\n",
    "            wins += 1\n",
    "        total_return += episode_return  # Track total return\n",
    "    return wins, total_return / num_episodes  # Return win count and average return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a6ac572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration Results:\n",
      "Wins: 717/1000 episodes\n",
      "Average Return: 0.717\n",
      "Time Taken: 0.135079 seconds\n",
      "\n",
      "Value Iteration Results:\n",
      "Wins: 741/1000 episodes\n",
      "Average Return: 0.741\n",
      "Time Taken: 0.101533 seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=True)\n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n \n",
    "\n",
    "    # Initialize an equal probability policy\n",
    "    initial_policy = np.ones((nS, nA)) / nA\n",
    "    discount_factor = 0.99  # Discount rate for future rewards\n",
    "    theta = 1e-8  # Convergence threshold\n",
    "    max_iterations = 1000  # Max iterations for algorithms\n",
    "\n",
    "    start_time = time.time()\n",
    "    V_policy, policy_policy = policy_iteration(initial_policy.copy(), env, discount_factor, theta, max_iterations)\n",
    "    policy_time = time.time() - start_time\n",
    "    wins_policy, avg_return_policy = evaluate_policy(env, policy_policy, num_episodes=1000)\n",
    "\n",
    "    print(\"Policy Iteration Results:\")\n",
    "    print(f\"Wins: {wins_policy}/1000 episodes\")\n",
    "    print(f\"Average Return: {avg_return_policy:.3f}\")\n",
    "    print(f\"Time Taken: {policy_time:.6f} seconds\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    V_value, policy_value = value_iteration(env, discount_factor, theta, max_iterations)\n",
    "    value_time = time.time() - start_time\n",
    "    wins_value, avg_return_value = evaluate_policy(env, policy_value, num_episodes=1000)\n",
    "\n",
    "    print(\"\\nValue Iteration Results:\")\n",
    "    print(f\"Wins: {wins_value}/1000 episodes\")\n",
    "    print(f\"Average Return: {avg_return_value:.3f}\")\n",
    "    print(f\"Time Taken: {value_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc8a894",
   "metadata": {},
   "source": [
    "### **Conclusion**\n",
    "- **Value Iteration** performed better with **more wins (741 vs. 717)** and **higher average return**.\n",
    "- **Value Iteration was faster**, converging in **less time** than Policy Iteration.\n",
    "\n",
    "ðŸ”¹ **Final Verdict**: **Value Iteration** is more efficient in this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cca8fac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
