{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "751e225e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gym in c:\\users\\mca\\appdata\\roaming\\python\\python39\\site-packages (0.26.2)\n",
      "Requirement already satisfied: importlib_metadata>=4.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (4.11.3)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in c:\\users\\mca\\appdata\\roaming\\python\\python39\\site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.21.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib_metadata>=4.8.0->gym) (3.7.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12dc80f",
   "metadata": {},
   "source": [
    "# CartPole-v0 Environment Task\n",
    "\n",
    "## Problem Overview\n",
    "\n",
    "The **CartPole-v0** environment is a classic control problem in reinforcement learning. The goal of the agent is to balance a pole attached to a cart, which moves along a track. The agent controls the movement of the cart and aims to keep the pole upright.\n",
    "\n",
    "### Key Components of the CartPole-v0 Environment:\n",
    "- **State Space**: The environment's state is represented by four variables:\n",
    "  1. Cart position\n",
    "  2. Cart velocity\n",
    "  3. Pole angle\n",
    "  4. Pole velocity at the tip\n",
    "- **Action Space**: The agent can take one of two actions:\n",
    "  1. Move the cart to the left.\n",
    "  2. Move the cart to the right.\n",
    "- **Reward System**: The agent receives a positive reward (usually +1) for every step that the pole remains balanced. If the pole falls, the episode ends, and no more rewards are earned.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257224e1",
   "metadata": {},
   "source": [
    "## Steps to Implement:\n",
    "\n",
    "### 1. Implement the CartPole Environment for a Certain Number of **Steps**\n",
    "   - This task involves running the simulation for a fixed number of **steps** within a single episode. Each step corresponds to an action taken by the agent.\n",
    "   - **Steps** mean a set number of actions, and once this number is reached or of pole falls, the episode will end. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e69087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eaa85b77",
   "metadata": {},
   "source": [
    "### 2. Implement the CartPole Environment for a Certain Number of **Episodes**\n",
    "   - An **episode** is a complete run of the environment, starting from the beginning and continuing until the pole falls or the maximum step limit (typically 200) is reached.\n",
    "   - You need to run the environment for a set number of **episodes**. Each episode will be independent, meaning it starts fresh and ends when the pole falls or after 200 steps.(If the agent balances the pole for 200 steps in the first episode, the episode ends with a total reward of 200.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc0aef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c7814ed",
   "metadata": {},
   "source": [
    "### 3. Compare and Comment on the Rewards Earned for Both Approaches\n",
    "   - After running the environment using both the **steps-based** and **episodes-based** approaches, you will compare the **total rewards** accumulated in each case.\n",
    "   - **Reward Mechanism**:\n",
    "     - The agent earns a reward of **+1** for each step where the pole is balanced.\n",
    "     - If the pole falls, the episode ends, and no additional rewards are earned.\n",
    "   - Compare the rewards for running for a number of **steps** versus running for a number of **episodes**:\n",
    "     - Does running for more episodes yield higher rewards?\n",
    "     - Does focusing on a specific number of steps affect the agentâ€™s performance?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32908264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de049d5b",
   "metadata": {},
   "source": [
    "### 4. Plot the Cumulative Reward\n",
    "   - After completing both approaches (steps-based and episodes-based), track the **cumulative reward** over time.\n",
    "   - The **cumulative reward** reflects how well the agent is performing as it learns to keep the pole balanced. This will provide insights into the agent's learning progress.\n",
    "   - Use a plot to visualize the cumulative rewards for each approach over time and compare the learning performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51488b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "878f6915",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "- **Steps-based approach**: You specify how many total steps you want to run, and each episode ends when the steps are finished, or the pole falls before reaching that number.\n",
    "- **Episodes-based approach**: You specify how many total episodes you want to run, and each episode is independent. The episode ends when the pole falls or after the maximum step limit is reached, and the agent has a chance to start fresh each time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98792c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
